{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'BRCA1': 264, 'TP53': 163, 'EGFR': 141, 'PTEN': 126, 'BRCA2': 125, 'KIT': 99, 'BRAF': 93, 'ERBB2': 69, 'ALK': 69, 'PDGFRA': 60, 'PIK3CA': 56, 'CDKN2A': 52, 'FGFR2': 50, 'FLT3': 49, 'TSC2': 47, 'MTOR': 45, 'KRAS': 44, 'MAP2K1': 43, 'VHL': 41, 'RET': 40, 'FGFR3': 39, 'MLH1': 35, 'SMAD4': 33, 'MET': 33, 'JAK2': 33, 'NOTCH1': 31, 'AKT1': 28, 'ROS1': 26, 'ABL1': 26, 'PTPN11': 26, 'CBL': 25, 'CTNNB1': 25, 'PIK3R1': 24, 'RUNX1': 24, 'PTPRT': 23, 'SMAD3': 23, 'PDGFRB': 23, 'HRAS': 22, 'NFE2L2': 22, 'MSH2': 21, 'SMO': 21, 'ERBB4': 20, 'TSC1': 20, 'SPOP': 20, 'AR': 20, 'SMAD2': 19, 'FBXW7': 19, 'ERCC2': 19, 'JAK1': 18, 'RHOA': 17, 'SF3B1': 16, 'ESR1': 16, 'PMS2': 16, 'IDH1': 16, 'NF1': 16, 'NTRK1': 16, 'TET2': 14, 'KEAP1': 14, 'MSH6': 13, 'FGFR1': 13, 'CCND1': 13, 'STK11': 13, 'PPP2R1A': 12, 'CARD11': 12, 'RAF1': 12, 'NRAS': 12, 'ERBB3': 11, 'EWSR1': 11, 'AKT2': 11, 'MAP2K2': 11, 'NF2': 11, 'PIK3CB': 10, 'POLE': 10, 'CDK12': 10, 'RB1': 10, 'DICER1': 9, 'CDH1': 9, 'EP300': 9, 'BAP1': 9, 'FANCA': 9, 'MAP2K4': 9, 'DDR2': 9, 'TERT': 8, 'TGFBR1': 8, 'EPAS1': 8, 'ETV6': 8, 'FOXA1': 8, 'EZH2': 8, 'PIM1': 8, 'PIK3R2': 8, 'MYC': 8, 'STAT3': 8, 'CCND3': 7, 'CHEK2': 7, 'CDKN2B': 7, 'BRIP1': 7, 'CREBBP': 7, 'ARAF': 7, 'SOX9': 7, 'KDR': 7, 'RAC1': 7, 'NKX2-1': 6, 'TMPRSS2': 6, 'ELF3': 6, 'ERCC4': 6, 'SMARCA4': 6, 'MEF2B': 6, 'ATM': 6, 'SOS1': 6, 'B2M': 6, 'CASP8': 6, 'TGFBR2': 5, 'KDM5C': 5, 'APC': 5, 'IDH2': 5, 'CTCF': 5, 'AGO2': 5, 'BCOR': 5, 'CIC': 5, 'PRDM1': 5, 'PTPRD': 5, 'GNAS': 5, 'MED12': 5, 'NTRK3': 5, 'RIT1': 4, 'CDKN1B': 4, 'ERG': 4, 'MAP3K1': 4, 'KMT2C': 4, 'FAT1': 4, 'YAP1': 4, 'MPL': 4, 'KNSTRN': 4, 'PPP6C': 4, 'NFKBIA': 4, 'AKT3': 4, 'RASA1': 4, 'RHEB': 3, 'RAD50': 3, 'PBRM1': 3, 'H3F3A': 3, 'CDK4': 3, 'CDKN1A': 3, 'RBM10': 3, 'ETV1': 3, 'ACVR1': 3, 'U2AF1': 3, 'PIK3CD': 3, 'KMT2A': 3, 'FGFR4': 3, 'CARM1': 3, 'SETD2': 3, 'IGF1R': 3, 'TET1': 3, 'NUP93': 3, 'MYD88': 3, 'GATA3': 3, 'AURKA': 3, 'BTK': 3, 'DNMT3A': 3, 'HNF1A': 3, 'NTRK2': 3, 'CCNE1': 2, 'KMT2D': 2, 'ARID2': 2, 'BRD4': 2, 'TP53BP1': 2, 'CDK6': 2, 'ERCC3': 2, 'HLA-A': 2, 'RXRA': 2, 'ARID1B': 2, 'KDM6A': 2, 'XPO1': 2, 'XRCC2': 2, 'SMARCB1': 2, 'FOXP1': 2, 'MAPK1': 2, 'MYCN': 2, 'ATRX': 2, 'SRC': 2, 'PTCH1': 2, 'FOXL2': 2, 'BCL10': 2, 'MGA': 2, 'NOTCH2': 2, 'DNMT3B': 2, 'RAD21': 2, 'RAD51C': 2, 'NPM1': 2, 'RAB35': 2, 'BCL2L11': 2, 'NSD1': 2, 'FAM58A': 1, 'SHOC2': 1, 'SHQ1': 1, 'CCND2': 1, 'RYBP': 1, 'LATS1': 1, 'LATS2': 1, 'EIF1AX': 1, 'HIST1H1C': 1, 'ERRFI1': 1, 'PAK1': 1, 'ASXL2': 1, 'EPCAM': 1, 'AURKB': 1, 'CDK8': 1, 'CDKN2C': 1, 'ASXL1': 1, 'CEBPA': 1, 'HLA-B': 1, 'IKZF1': 1, 'MDM2': 1, 'ARID1A': 1, 'MDM4': 1, 'KLF4': 1, 'AXIN1': 1, 'MEN1': 1, 'FANCC': 1, 'ARID5B': 1, 'FGF3': 1, 'FGF4': 1, 'SDHB': 1, 'RAD54L': 1, 'SDHC': 1, 'VEGFA': 1, 'PMS1': 1, 'FOXO1': 1, 'RRAS2': 1, 'FLT1': 1, 'SRSF2': 1, 'WHSC1': 1, 'PPM1D': 1, 'PIK3R3': 1, 'RICTOR': 1, 'NCOR1': 1, 'IKBKE': 1, 'CTLA4': 1, 'STAG2': 1, 'IL7R': 1, 'KMT2B': 1, 'ATR': 1, 'AXL': 1, 'MYOD1': 1, 'BARD1': 1, 'BCL2': 1, 'RNF43': 1, 'INPP4B': 1, 'WHSC1L1': 1, 'JUN': 1, 'PAX8': 1, 'GLI1': 1, 'FUBP1': 1, 'GNA11': 1, 'GNAQ': 1, 'FGF19': 1, 'RAD51B': 1, 'RAD51D': 1, 'TCF3': 1, 'TCF7L2': 1, 'RARA': 1, 'KDM5A': 1, 'DUSP4': 1}) \n",
      " 264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text analysis helper libraries\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "# Text analysis helper libraries for word frequency etc..\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Word cloud visualization libraries\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "#from wordcloud import WordCloud, ImageColorGenerator\n",
    "from collections import Counter\n",
    "\n",
    "# Word2Vec related libraries\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#NLP\n",
    "\n",
    "#Importing dataset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#Importing Dataset\n",
    "#import os\n",
    "#os.chdir('D:\\Py-R\\cancer')\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\aprad\\\\Documents\\\\Cancer_data')\n",
    "source= 'C:\\\\Users\\\\aprad\\\\Documents\\\\Cancer_data'\n",
    "\n",
    "train_variant = pd.read_csv(source+'/training_variants')\n",
    "test_variant = pd.read_csv(source+'/test_variants')\n",
    "\n",
    "train_text = pd.read_csv(source+'/training_text',sep = '\\|\\|', engine= 'python', header=None, \n",
    "                     skiprows=1, names=[\"ID\",\"Text\"])\n",
    "test_text = pd.read_csv(source+'/test_text',sep = '\\|\\|', engine= 'python', header=None, \n",
    "                     skiprows=1, names=[\"ID\",\"Text\"])\n",
    "\n",
    "train = pd.merge(train_variant, train_text, how = 'left', on = 'ID').fillna('')\n",
    "test = pd.merge(test_variant, test_text, how = 'left', on = 'ID').fillna('')\n",
    "\n",
    "\n",
    "#Data Exploration\n",
    "train.Gene.nunique()\n",
    "train['Gene'].unique()\n",
    "\n",
    "k = train.groupby('Gene')['Gene'].count()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(k, bins=150,log=True)\n",
    "plt.xlabel('Number of times Gene appared')\n",
    "plt.ylabel('Log of count')\n",
    "plt.show()\n",
    "\n",
    "#count Gene\n",
    "from collections import Counter\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.countplot((train['Gene']))\n",
    "plt.xticks()\n",
    "genecount = Counter(train['Gene'])\n",
    "print(genecount,'\\n',len(genecount))\n",
    "\n",
    "#train.Variation.nunique()\n",
    "#train['Variation'].unique()\n",
    "#k = train.groupby('Variation')['Variation'].count()\n",
    "\n",
    "plt.figure(figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Doc 0\n",
      "[('cyclin', 91), ('.', 81), ('cdk10', 65), ('fig.', 63), ('express', 59), ('cell', 58), ('protein', 50), ('ets2', 46), ('level', 41), (',', 39)]\n",
      "\n",
      " Doc 1\n",
      "[('c-cbl', 139), ('mutat', 103), ('.', 82), ('cell', 78), ('lung', 56), (',', 48), ('egfr', 38), ('sampl', 33), ('1', 33), ('met', 32)]\n",
      "\n",
      " Doc 2\n",
      "[('c-cbl', 139), ('mutat', 103), ('.', 82), ('cell', 78), ('lung', 56), (',', 48), ('egfr', 38), ('sampl', 33), ('1', 33), ('met', 32)]\n",
      "\n",
      " Doc 3\n",
      "[('cbl', 101), ('mutat', 94), ('case', 65), ('.', 41), ('upn', 38), ('aupd', 37), ('cell', 37), ('figur', 35), ('2', 30), ('analysi', 25)]\n",
      "\n",
      " Doc 4\n",
      "[('mutat', 150), ('cbl', 113), ('.', 95), ('cancer', 63), ('bind', 62), ('activ', 58), ('protein', 45), ('effect', 40), ('cell', 38), ('stabil', 35)]\n",
      "\n",
      " Doc 5\n",
      "[('mutat', 150), ('cbl', 113), ('.', 95), ('cancer', 63), ('bind', 62), ('activ', 58), ('protein', 45), ('effect', 40), ('cell', 38), ('stabil', 35)]\n",
      "\n",
      " Doc 6\n",
      "[('mutat', 150), ('cbl', 113), ('.', 95), ('cancer', 63), ('bind', 62), ('activ', 58), ('protein', 45), ('effect', 40), ('cell', 38), ('stabil', 35)]\n",
      "\n",
      " Doc 7\n",
      "[('cbl', 231), ('mutat', 201), ('cell', 172), ('flt3', 153), ('patient', 152), ('.', 147), (',', 85), ('mutant', 72), ('express', 60), ('exon', 57)]\n",
      "\n",
      " Doc 8\n",
      "[('cbl', 250), ('mutat', 185), ('.', 173), ('cell', 131), ('mutant', 96), ('activ', 93), ('phosphoryl', 82), ('express', 80), ('bind', 71), ('cancer', 63)]\n",
      "\n",
      " Doc 9\n",
      "[('cbl', 137), ('cell', 93), ('.', 78), ('express', 76), ('phosphoryl', 75), ('mutant', 63), ('gm-csfr', 58), ('gm-csf', 57), ('c', 52), ('enhanc', 50)]\n",
      "[[0.         0.97253148 0.23277141 ... 0.         0.         0.        ]\n",
      " [0.27582062 0.96120913 0.         ... 0.         0.         0.        ]\n",
      " [0.27582062 0.96120913 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#cleaning of data\n",
    "def cleantext(train):\n",
    "    corpus = []\n",
    "    for i in range(0,train.shape[0]):\n",
    "        review = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\",\" \",train['Text'][i])\n",
    "        review = review.lower().split()\n",
    "        ps = PorterStemmer()\n",
    "        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corp_train = cleantext(train)\n",
    "corp_test = cleantext(test)\n",
    "\n",
    "# Determine lenght of text\n",
    "def textlen(train):\n",
    "    k = train['Text'].apply(lambda x: len(str(x).split()))\n",
    "    l = train['Text'].apply(lambda x: len(str(x)))\n",
    "    return k, l\n",
    "\n",
    "train['Text_no_word'], train['Text_no_char'] = textlen(train)\n",
    "test['Text_no_word'], test['Text_no_char'] = textlen(test)\n",
    "\n",
    "#\n",
    "for i in range(10):\n",
    "    print('\\n Doc', str(i))\n",
    "    stopcheck = Counter(corp_train[i].split())\n",
    "    print(stopcheck.most_common()[:10])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "\tmin_df=1, max_features=1600, strip_accents='unicode',lowercase =True,\n",
    "\tanalyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n",
    "\tsmooth_idf=True, sublinear_tf=True, stop_words = 'english'\n",
    ").fit(train)\n",
    "\n",
    "X_train = tfidf.transform(corp_train).toarray()\n",
    "print(X_train)\n",
    "\n",
    "X_test = tfidf.transform(corp_test).toarray()\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "y_train = train['Class']\n",
    "\n",
    "#Converting to categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "y_train=le.fit_transform(y_train)\n",
    "#onh = OneHotEncoder(categorical_features = [0])\n",
    "#y_train =onh.fit_transform(y_train).toarray()\n",
    "\n",
    "# Preparing model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=5000,max_depth=8,min_samples_split=2)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred=classifier.predict_proba(X_test)\n",
    "y=pd.DataFrame(y_pred)\n",
    "#np.mean(y_pred==y_train)\n",
    "\n",
    "submit = pd.DataFrame(test.ID)\n",
    "submit = submit.join(pd.DataFrame(y_pred))\n",
    "submit.columns = ['ID', 'class1','class2','class3','class4','class5','class6','class7','class8','class9']\n",
    "\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
